{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "kKCHcjbdZ0vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re\n",
        "import string\n"
      ],
      "metadata": {
        "id": "FA8BfNZwPwZf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fuction for cleaning the text**"
      ],
      "metadata": {
        "id": "Ghm8D4draCJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    clean_text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
        "    # Remove extra whitespaces\n",
        "    clean_text = \" \".join(clean_text.split())\n",
        "    # Remove punctuation\n",
        "    clean_text = clean_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "gGL3RgdEaBSp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**URL to extract data**"
      ],
      "metadata": {
        "id": "EdX92RhQaUD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.dawn.com/latest-news\"\n",
        "r = requests.get(URL)\n",
        "soup = BeautifulSoup(r.content, 'html5lib')\n",
        "news = []\n",
        "table = soup.find('div', attrs = {'class':'tabs__pane active'})\n",
        "for row in table.findAll('div', attrs = {'class':'flex flex-col sm:flex-row'}):\n",
        "    new = {}\n",
        "    new['headline'] = clean_text(row.h2.text)\n",
        "    new['description'] = clean_text(row.find('div', attrs = {'class':'story__excerpt font-georgia font-noto text-3.5 text-gray-700 overflow-hidden pb-1 px-0 sm:px-2 mt-0'}).text)\n",
        "    new['url'] = row.a['href']\n",
        "    try:\n",
        "        new['img'] = row.img['src']\n",
        "    except:\n",
        "        continue\n",
        "    new['Published Date'] = clean_text(row.find('span', attrs = {'class':'timestamp__calendar'}).text)\n",
        "    news.append(new)\n"
      ],
      "metadata": {
        "id": "II9molbkZ9p0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data saving in csv file**"
      ],
      "metadata": {
        "id": "e5yp3cckaYFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the cleaned data into a CSV file\n",
        "filename = 'latestnews_dawn.csv'\n",
        "with open(filename, 'w', newline='') as f:\n",
        "    w = csv.DictWriter(f,['headline','description','url','img','Published Date'])\n",
        "    w.writeheader()\n",
        "    for n in news:\n",
        "        w.writerow(n)"
      ],
      "metadata": {
        "id": "erH32gjzaSvX"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}