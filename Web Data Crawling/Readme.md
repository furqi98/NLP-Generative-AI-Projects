
# 🕷️ Web Data Crawling - Assignment 1  

## 📌 Overview  
This project involves developing a **web crawler** to extract textual data from **Pakistani-origin news websites**, specifically **Dawn.com**. The goal is to **scrape, clean, and store structured data** from online sources using Python-based web scraping libraries.  

## 🚀 Features  
✅ **Web Crawling**: Extract news headlines, descriptions, publication dates, and images.  
✅ **Text Cleaning**: Remove HTML tags, punctuation, and extra spaces for clean text.  
✅ **Data Storage**: Store extracted content in a **CSV file** for further analysis.  
✅ **Colab Notebook**: A step-by-step **Google Colab Notebook** demonstrating the entire crawling process.  

## 🛠️ Technologies Used  
- **Python**  
- **BeautifulSoup** – For web scraping  
- **Requests** – For making HTTP requests  
- **CSV Handling** – To store structured data  

## 📥 Installation & Setup  

### 1️⃣ Clone the Repository  
```bash
git clone https://github.com/your-username/Web-Data-Crawling.git
cd Web-Data-Crawling
```

### 2️⃣ Install Dependencies  
```bash
pip install -r requirements.txt
```

### 3️⃣ Run the Crawler  
```bash
python "Web Data Crawler.ipynb"
```


## 📊 Data Collected  
✅ **2GB+ of text data** from multiple news articles  
✅ Stored in **CSV format** for further processing  
✅ Covers **headlines, descriptions, dates, and URLs**  

 

