
# ğŸ•·ï¸ Web Data Crawling - Assignment 1  

## ğŸ“Œ Overview  
This project involves developing a **web crawler** to extract textual data from **Pakistani-origin news websites**, specifically **Dawn.com**. The goal is to **scrape, clean, and store structured data** from online sources using Python-based web scraping libraries.  

## ğŸš€ Features  
âœ… **Web Crawling**: Extract news headlines, descriptions, publication dates, and images.  
âœ… **Text Cleaning**: Remove HTML tags, punctuation, and extra spaces for clean text.  
âœ… **Data Storage**: Store extracted content in a **CSV file** for further analysis.  
âœ… **Colab Notebook**: A step-by-step **Google Colab Notebook** demonstrating the entire crawling process.  

## ğŸ› ï¸ Technologies Used  
- **Python**  
- **BeautifulSoup** â€“ For web scraping  
- **Requests** â€“ For making HTTP requests  
- **CSV Handling** â€“ To store structured data  

## ğŸ“¥ Installation & Setup  

### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/your-username/Web-Data-Crawling.git
cd Web-Data-Crawling
```

### 2ï¸âƒ£ Install Dependencies  
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the Crawler  
```bash
python "Web Data Crawler.ipynb"
```


## ğŸ“Š Data Collected  
âœ… **2GB+ of text data** from multiple news articles  
âœ… Stored in **CSV format** for further processing  
âœ… Covers **headlines, descriptions, dates, and URLs**  

 

