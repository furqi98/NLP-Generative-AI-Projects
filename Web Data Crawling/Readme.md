
# ğŸ•·ï¸ Web Data Crawling - Assignment 1  

## ğŸ“Œ Overview  
This project involves developing a **web crawler** to extract textual data from **Pakistani-origin news websites**, specifically **Dawn.com**. The goal is to **scrape, clean, and store structured data** from online sources using Python-based web scraping libraries.  

## ğŸš€ Features  
âœ… **Web Crawling**: Extract news headlines, descriptions, publication dates, and images.  
âœ… **Text Cleaning**: Remove HTML tags, punctuation, and extra spaces for clean text.  
âœ… **Data Storage**: Store extracted content in a **CSV file** for further analysis.  
âœ… **Colab Notebook**: A step-by-step **Google Colab Notebook** demonstrating the entire crawling process.  

## ğŸ› ï¸ Technologies Used  
- **Python**  
- **BeautifulSoup** â€“ For web scraping  
- **Requests** â€“ For making HTTP requests  
- **CSV Handling** â€“ To store structured data  

## ğŸ“¥ Installation & Setup  

1ï¸âƒ£ Install Jupyter Notebook (if not installed)

bash
Copy
Edit
pip install notebook
2ï¸âƒ£ Launch Jupyter Notebook

bash
Copy
Edit
jupyter notebook
3ï¸âƒ£ Open the .ipynb file and run the cells in sequence.
```


## ğŸ“Š Data Collected  
âœ… **2GB+ of text data** from multiple news articles  
âœ… Stored in **CSV format** for further processing  
âœ… Covers **headlines, descriptions, dates, and URLs**  

 

